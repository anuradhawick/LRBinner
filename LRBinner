#!/usr/bin/python3
import argparse
import os
import sys
import time
import logging
from Bio import SeqIO
import numpy as np

from mbcclr_utils import runners_utils
from mbcclr_utils import ae_utils
from mbcclr_utils import cluster_utils


def main():
    parser = argparse.ArgumentParser(description="""LRBinner Help. A tool developed for binning of metagenomics long reads (PacBio/ONT). \
            Tool utilizes composition and coverage profiles of reads based on k-mer frequencies to perform dimension reduction via a deep variational auto-encoder. \
            Dimension reduced reads are then clustered using a novel distance histogram based clustering algorithm. Minimum RAM requirement is 9GB.""")

    parser.add_argument('--reads-path', '-r',
                        help="Reads path for binning",
                        type=str,
                        required=True)
    parser.add_argument('--k-size', '-k',
                        help="k value for k-mer frequency vector. Choose between 3 and 5.",
                        type=int,
                        required=False,
                        choices=[3, 4, 5],
                        default=4)
    parser.add_argument('--bin-size', '-bs',
                        help="Bin size for the coverage histogram.",
                        type=int,
                        required=False,
                        default=10)
    parser.add_argument('--bin-count', '-bc',
                        help="Number of bins for the coverage histogram.",
                        type=int,
                        required=False,
                        default=32)
    parser.add_argument('--ae-epochs',
                        help="Epochs for the auto_encoder.",
                        type=int,
                        required=False,
                        default=200)
    parser.add_argument('--ae-dims',
                        help="Size of the latent dimension.",
                        type=int,
                        required=False,
                        default=8)
    parser.add_argument('--ae-hidden',
                        help="Hidden layer sizes eg: 128,128",
                        type=str,
                        required=False,
                        default="128,128")
    parser.add_argument('--min-bin-size', '-mbs',
                        help="The minimum number of reads a bin should have.",
                        type=int,
                        required=False,
                        default=10000)
    parser.add_argument('--bin-iterations', '-bit',
                        help="Number of iterations for cluster search. Use 0 for exhaustive search.",
                        type=int,
                        required=False,
                        default=1000)
    parser.add_argument('--threads', '-t',
                        help="Thread count for computations",
                        type=int,
                        default=8,
                        required=False)
    parser.add_argument('--separate-reads', '-sep',
                        help="Flag to separate reads into bins detected. Avaialbe in folder named 'binned'.",
                        action='store_true')
    parser.add_argument('--resume',
                        action='store_true',
                        help='Continue from the last step or the binning step (which ever comes first). Can save time needed to run DSK and obtain k-mers.'
                        )
    parser.add_argument('--output', '-o', metavar='<DEST>',
                        help="Output directory", type=str, required=True)
    parser.add_argument('--version', '-v',
                        action='version',
                        help="Show version.",
                        version='%(prog)s 0.1')

    # command line args
    args = parser.parse_args()

    reads_path = args.reads_path
    threads = args.threads
    bin_size = args.bin_size
    bin_count = args.bin_count
    k_size = args.k_size
    epochs = args.ae_epochs
    dims = args.ae_dims
    hidden = list(map(int, args.ae_hidden.split(",")))
    binreads = args.separate_reads
    resume = args.resume
    min_cluster_size = max(args.min_bin_size, 1)
    iterations = max(args.bin_iterations, 0)
    output = args.output

    checkpoints_path = f"{output}/checkpoints"

    logger = logging.getLogger('LRBinner')
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    consoleHeader = logging.StreamHandler()
    consoleHeader.setFormatter(formatter)
    consoleHeader.setLevel(logging.INFO)
    logger.addHandler(consoleHeader)

    start_time = time.time()

    # Validation of inputs
    if not reads_path.split(".")[-1].lower() in ['fq', 'fasta', 'fa', 'fastq']:
        logger.error(
            "Unable to detect file type of reads. Please use either FASTA of FASTQ. Good Bye!")
        sys.exit(1)

    if threads <= 0:
        print("Minimum number of threads is 1. Using thread count 1 and continue")
        threads = 1

    if not os.path.isfile(reads_path):
        print("Failed to open reads file")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if not os.path.exists(output):
        os.makedirs(output)
        os.makedirs(f"{output}/profiles")

    # Validation of inputs end

    fileHandler = logging.FileHandler(f"{output}/LRBinner.log")
    fileHandler.setLevel(logging.DEBUG)
    fileHandler.setFormatter(formatter)
    logger.addHandler(fileHandler)

    if not resume:
        data = {}
        # general
        data['reads'] = reads_path
        data['threads'] = threads
        data['output'] = output

        # vectorise related
        data['kmer'] = k_size

        # coverage profile related
        data['bin_size'] = bin_size
        data['bin_count'] = bin_count

        # ae related
        data['ae_dims'] = dims
        data['ae_hidden'] = hidden
        data['epochs'] = epochs

        data['completed'] = set()

        runners_utils.checkpoint(data, checkpoints_path)

    # running program
    start_time = time.time()
    logger.info("Command " + " ".join(sys.argv))

    if resume:
        logger.info("Resuming the program from previous checkpoints")
        data = runners_utils.load_checkpoints(checkpoints_path)
        logger.debug(str(data))

    data_changed = False
    if resume and ("kmer_vecs" not in data['completed'] or data['kmer'] != k_size) or not resume:
        logger.info("Counting k-mers")
        runners_utils.run_kmers(reads_path, output, k_size, threads)
        data['kmer'] = k_size
        data['completed'].add('kmer_vecs')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting k-mers complete")
        data_changed = True

    if resume and ("15mer-counts" not in data['completed']) or not resume:
        logger.info("Counting 15-mers")
        runners_utils.run_15mer_counts(reads_path, output, threads)
        data['completed'].add('15mer-counts')
        if '15mer-vecs' in data['completed']:
            data['completed'].remove('15mer-vecs')  # needs needs next step
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting 15-mers complete")
        data_changed = True

    if resume and ("15mer-vecs" not in data['completed'] or
                   data['bin_size'] != bin_size or data['bin_count'] != bin_count) or not resume:
        logger.info("Counting 15-mer profiles")
        runners_utils.run_15mer_vecs(
            reads_path, output, bin_size, bin_count, threads)
        data['bin_size'] = bin_size
        data['bin_count'] = bin_count
        data['completed'].add('15mer-vecs')
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting 15-mer profiles complete")
        data_changed = True

    if data_changed:
        logger.info("Profiles saving as numpy arrays")
        comp_profiles = np.array([np.array(list(map(float, line.strip().split()))) for line in open(
            f"{output}/profiles/com_profs") if len(line.strip()) > 0])
        cov_profiles = np.array([np.array(list(map(float, line.strip().split()))) for line in open(
            f"{output}/profiles/cov_profs") if len(line.strip()) > 0])

        np.save(f"{output}/profiles/com_profs", comp_profiles)
        np.save(f"{output}/profiles/cov_profs", cov_profiles)

        del comp_profiles
        del cov_profiles

        logger.info("Profiles saving as numpy arrays complete")

    if resume and ("encode" not in data['completed'] or
                   data['ae_dims'] != dims or
                   data['ae_hidden'] != hidden or
                   data['epochs'] != epochs or
                   data_changed) or not resume:

        logger.info(f"VAE Training information")
        logger.info(f"\tDimensions {dims}")
        logger.info(f"\tHidden Layers {hidden}")
        logger.info(f"\tEpochs {epochs}")

        ae_utils.vae_encode(
            output,
            dims,
            hidden,
            epochs)
            
        data['ae_dims'] = dims
        data['ae_hidden'] = hidden
        data['epochs'] = epochs
        data['completed'].add('encode')

        runners_utils.checkpoint(data, checkpoints_path)
        logger.info(f"VAE Training complete")

    cluster_utils.perform_binning(
        output, iterations, min_cluster_size, binreads, reads_path)

    end_time = time.time()
    time_taken = end_time - start_time
    logger.info(
        f"Program Finished!. Please find the output in bins.txt")
    logger.info(f"Total time consumed = {time_taken:10.2f} seconds")
    logger.info(
        f"Thank you for using LRBinner. Feedback will be much appreciated!")

    logger.removeHandler(fileHandler)
    logger.removeHandler(consoleHeader)


if __name__ == '__main__':
    main()
