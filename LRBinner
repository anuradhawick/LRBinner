#!/usr/bin/python3
import argparse
import os
import sys
import time
import logging
from Bio import SeqIO

from mbcclr_utils import runners_utils
from mbcclr_utils import ae_utils


def main():
    parser = argparse.ArgumentParser(description="""LRBinner Help. A tool developed for binning of metagenomics long reads (PacBio/ONT). \
            Tool utilizes composition and coverage profiles of reads based on k-mer frequencies to perform dimension reduction via a deep variational auto-encoder. \
            dimension reduced reads are then clustered using a randomised clustering algorithm. Minimum RAM requirement is 9GB.""")

    parser.add_argument('--reads-path', '-r',
                        help="Reads path for binning",
                        type=str,
                        required=True)
    parser.add_argument('--threads', '-t',
                        help="Thread count for computation",
                        type=int,
                        default=8,
                        required=False)
    parser.add_argument('--bin-size', '-bs',
                        help="Bin size for the coverage histogram.",
                        type=int,
                        required=False,
                        default=10)
    parser.add_argument('--bin-count', '-bc',
                        help="Number of bins for the coverage histogram.",
                        type=int,
                        required=False,
                        default=32)                        
    parser.add_argument('--k-mer-vector', '-k',
                        help="k value for k-mer frequency vector. Choose between 3 and 5.",
                        type=int,
                        required=False,
                        choices=[3, 4, 5, 6, 7],
                        default=3)
    parser.add_argument('--max-memory', '-m',
                        help="Default 5000. DSK k-mer counter accepts a max memory parameter. However, the complete pipeline requires 5GB+ RAM. \
                            This is only to make DSK step faster, should you have more RAM.",
                        type=int,
                        required=False,
                        default=5000)
    parser.add_argument('--ae-epochs',
                        help="Epochs for the auto_encoder.",
                        type=int,
                        required=False,
                        default=200)
    parser.add_argument('--ae-dims',
                        help="Size of the latent dimension.",
                        type=int,
                        required=False,
                        default=8)
    parser.add_argument('--ae-hidden',
                        help="Hidden layer sizes eg: 128,128",
                        type=str,
                        required=False,
                        default="128,128")
    parser.add_argument('--resume',
                        action='store_true',
                        help='Continue from the last step or the binning step (which ever comes first). Can save time needed to run DSK and obtain k-mers.'
                        )
    parser.add_argument('--output', '-o', metavar='<DEST>',
                        help="Output directory", type=str, required=True)
    parser.add_argument('--version', '-v',
                        action='version',
                        help="Show version.",
                        version='%(prog)s 0.1')

    # command line args
    args = parser.parse_args()

    reads_path = args.reads_path
    threads = args.threads
    bin_size = args.bin_size
    bin_count = args.bin_count
    k_mer_vector = args.k_mer_vector
    max_memory = max(args.max_memory, 5000)
    epochs = args.ae_epochs
    dims = args.ae_dims
    hidden = list(map(int, args.ae_hidden.split(",")))
    resume = args.resume
    output = args.output

    checkpoints_path = f"{output}/checkpoints"

    logger = logging.getLogger('LRBinner')
    logger.setLevel(logging.DEBUG)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    consoleHeader = logging.StreamHandler()
    consoleHeader.setFormatter(formatter)
    consoleHeader.setLevel(logging.INFO)
    logger.addHandler(consoleHeader)

    start_time = time.time()

    # Validation of inputs
    if not reads_path.split(".")[-1].lower() in ['fq', 'fasta', 'fa', 'fastq']:
        logger.error(
            "Unable to detect file type of reads. Please use either FASTA of FASTQ. Good Bye!")
        sys.exit(1)

    if threads <= 0:
        print("Minimum number of threads is 1. Using thread count 1 and continue")
        threads = 1

    if not os.path.isfile(reads_path):
        print("Failed to open reads file")
        print("Exitting process. Good Bye!")
        sys.exit(1)

    if not os.path.exists(output):
        os.makedirs(output)
        os.makedirs(f"{output}/profiles")

    # Validation of inputs end

    fileHandler = logging.FileHandler(f"{output}/LRBinner.log")
    fileHandler.setLevel(logging.DEBUG)
    fileHandler.setFormatter(formatter)
    logger.addHandler(fileHandler)

    if not resume:
        data = {}
        data['r'] = reads_path
        data['k'] = k_mer_vector
        data['bs'] = bin_size
        data['bc'] = bin_count
        data['o'] = output
        data['t'] = threads
        data['completed'] = set()

        runners_utils.checkpoint(data, checkpoints_path)

    # running program
    start_time = time.time()

    # if resume:
    #     logger.info("Resuming the program from previous checkpoints")
    #     data = runners_utils.load_checkpoints(checkpoints_path)
    #     logger.debug(str(data))

    # if resume and "dsk" not in data['completed'] or not resume:
    #     logger.info("Running DSK k-mer counting")
    #     runners_utils.run_dsk(reads_path, output, 2, max_memory, threads)
    #     logger.info("Running DSK k-mer counting complete")
    #     data['completed'].add('dsk')
    #     runners_utils.checkpoint(data, checkpoints_path)
    #     logger.info("Running DSK k-mer complete")

    # return

    # if resume and "kmer_vecs" not in data['completed'] or not resume:
    #     logger.info("Counting k-mers")
    #     runners_utils.run_kmers(reads_path, f"{output}/profiles/com_profs", k_mer_vector, threads)
    #     data['completed'].add('kmer_vecs')
    #     runners_utils.checkpoint(data, checkpoints_path)
    #     logger.info("Counting k-mers complete")

    if resume and "15mers" not in data['completed'] or not resume:
        logger.info("Counting 15-mer profiles")
        data['completed'].add('15mers')
        dsk_file = f"{output}/DSK/15mersCounts"
        runners_utils.run_15mers(dsk_file, reads_path, f"{output}/profiles/cov_profs", bin_size, bin_count, threads)
        runners_utils.checkpoint(data, checkpoints_path)
        logger.info("Counting 15-mer profiles complete")

    return 

    end_time = time.time()
    time_taken = end_time - start_time
    logger.info(
        f"Program Finished!. Please find the output in {output}/final.txt")
    logger.info(f"Total time consumed = {time_taken:10.2f} seconds")

    logger.removeHandler(fileHandler)
    logger.removeHandler(consoleHeader)

    # TODO
    ae_utils.vae_encode(output, f'{output}/com_profs', f'{output}/cov_profs', latent_dims, hidden_layers, epochs)


if __name__ == '__main__':
    main()
